# coding=gbk
import torch
import matplotlib.pyplot as plt
from torch.autograd import Variable
import torch.nn.functional as F

"""pytorch-神经网络拟合曲线"""
'''
    Pytorch是一个拥有强力GPU加速的张量和动态构建网络的库，其主要构建是张量，所以可以把PyTorch当做Numpy
    来用,Pytorch的很多操作好比Numpy都是类似的，但是其能够在GPU上运行，所以有着比Numpy快很多倍的速度。
    训练完了，发现隐层越大，拟合的速度越是快，拟合的效果越是好
'''

def train():
    print('------ 构建数据集 -------')
    # torch.linspace是为了生成连续间断的数据，第一个参数表示起点，第二个参数表示终点，第三个参数表示将这个区间分成平均几份，即生成几个数据
    x = torch.linspace(-1, 1, 100)
    ''' x =
        tensor([-1.0000, -0.9798, -0.9596, -0.9394, -0.9192, -0.8990, -0.8788, -0.8586,
        -0.8384, -0.8182, -0.7980, -0.7778, -0.7576, -0.7374, -0.7172, -0.6970,
        -0.6768, -0.6566, -0.6364, -0.6162, -0.5960, -0.5758, -0.5556, -0.5354,
        -0.5152, -0.4949, -0.4747, -0.4545, -0.4343, -0.4141, -0.3939, -0.3737,
        -0.3535, -0.3333, -0.3131, -0.2929, -0.2727, -0.2525, -0.2323, -0.2121,
        -0.1919, -0.1717, -0.1515, -0.1313, -0.1111, -0.0909, -0.0707, -0.0505,
        -0.0303, -0.0101,  0.0101,  0.0303,  0.0505,  0.0707,  0.0909,  0.1111,
         0.1313,  0.1515,  0.1717,  0.1919,  0.2121,  0.2323,  0.2525,  0.2727,
         0.2929,  0.3131,  0.3333,  0.3535,  0.3737,  0.3939,  0.4141,  0.4343,
         0.4545,  0.4747,  0.4949,  0.5152,  0.5354,  0.5556,  0.5758,  0.5960,
         0.6162,  0.6364,  0.6566,  0.6768,  0.6970,  0.7172,  0.7374,  0.7576,
         0.7778,  0.7980,  0.8182,  0.8384,  0.8586,  0.8788,  0.8990,  0.9192,
         0.9394,  0.9596,  0.9798,  1.0000])
    '''
    x = torch.unsqueeze(x, dim=1)
    print(x)
    ''' x =
        tensor([[-1.0000],
        [-0.9798],
        [-0.9596],
        [-0.9394],
        [-0.9192],
        [-0.8990],
        [-0.8788],
        [-0.8586],
        [-0.8384],
        [-0.8182],
        [-0.7980],
        [-0.7778],
        [-0.7576],
        [-0.7374],
        [-0.7172],
        [-0.6970],
        [-0.6768],
        [-0.6566],
        [-0.6364],
        [-0.6162],
        [-0.5960],
        [-0.5758],
        [-0.5556],
        [-0.5354],
        [-0.5152],
        [-0.4949],
        [-0.4747],
        [-0.4545],
        [-0.4343],
        [-0.4141],
        [-0.3939],
        [-0.3737],
        [-0.3535],
        [-0.3333],
        [-0.3131],
        [-0.2929],
        [-0.2727],
        [-0.2525],
        [-0.2323],
        [-0.2121],
        [-0.1919],
        [-0.1717],
        [-0.1515],
        [-0.1313],
        [-0.1111],
        [-0.0909],
        [-0.0707],
        [-0.0505],
        [-0.0303],
        [-0.0101],
        [ 0.0101],
        [ 0.0303],
        [ 0.0505],
        [ 0.0707],
        [ 0.0909],
        [ 0.1111],
        [ 0.1313],
        [ 0.1515],
        [ 0.1717],
        [ 0.1919],
        [ 0.2121],
        [ 0.2323],
        [ 0.2525],
        [ 0.2727],
        [ 0.2929],
        [ 0.3131],
        [ 0.3333],
        [ 0.3535],
        [ 0.3737],
        [ 0.3939],
        [ 0.4141],
        [ 0.4343],
        [ 0.4545],
        [ 0.4747],
        [ 0.4949],
        [ 0.5152],
        [ 0.5354],
        [ 0.5556],
        [ 0.5758],
        [ 0.5960],
        [ 0.6162],
        [ 0.6364],
        [ 0.6566],
        [ 0.6768],
        [ 0.6970],
        [ 0.7172],
        [ 0.7374],
        [ 0.7576],
        [ 0.7778],
        [ 0.7980],
        [ 0.8182],
        [ 0.8384],
        [ 0.8586],
        [ 0.8788],
        [ 0.8990],
        [ 0.9192],
        [ 0.9394],
        [ 0.9596],
        [ 0.9798],
        [ 1.0000]])
    '''
    # torch.rand返回的是[0,1]之间的均匀分布   这里是使用一个计算式子来构造出一个关联结果，当然后期要学的也就是这个式子
    y = x.pow(2) + 0.2 * torch.rand(x.size())
    ''' y =
        tensor([[1.0735],
        [1.0514],
        [0.9401],
        [1.0167],
        [0.8518],
        [0.8506],
        [0.9467],
        [0.8677],
        [0.8629],
        [0.7805],
        [0.8066],
        [0.6846],
        [0.7683],
        [0.5503],
        [0.6885],
        [0.5676],
        [0.5455],
        [0.5523],
        [0.4728],
        [0.5555],
        [0.4834],
        [0.5236],
        [0.3451],
        [0.3560],
        [0.3215],
        [0.2849],
        [0.2520],
        [0.4050],
        [0.2455],
        [0.3137],
        [0.2368],
        [0.2754],
        [0.1617],
        [0.2418],
        [0.2812],
        [0.0881],
        [0.2736],
        [0.2132],
        [0.0677],
        [0.2382],
        [0.1176],
        [0.0831],
        [0.1978],
        [0.2164],
        [0.1286],
        [0.0467],
        [0.0912],
        [0.1369],
        [0.0187],
        [0.1789],
        [0.0593],
        [0.1913],
        [0.1756],
        [0.0453],
        [0.1933],
        [0.0491],
        [0.1641],
        [0.2049],
        [0.1205],
        [0.1825],
        [0.1306],
        [0.2438],
        [0.2180],
        [0.0870],
        [0.1929],
        [0.1873],
        [0.1249],
        [0.2328],
        [0.2723],
        [0.3063],
        [0.3662],
        [0.2071],
        [0.3108],
        [0.3104],
        [0.3108],
        [0.4468],
        [0.3065],
        [0.3613],
        [0.4461],
        [0.4686],
        [0.5061],
        [0.4099],
        [0.6274],
        [0.5258],
        [0.5701],
        [0.6032],
        [0.7062],
        [0.7416],
        [0.6908],
        [0.7228],
        [0.8512],
        [0.8806],
        [0.8383],
        [0.8128],
        [0.9666],
        [0.9623],
        [0.9437],
        [0.9281],
        [1.0641],
        [1.0782]])
    '''
    # Variable是将tensor封装了下，用于自动求导使用
    x, y = Variable(x),  Variable(y)
    # 绘图展示
    # plt.scatter(x.data.numpy(), y.data.numpy())
    ''' x.data.numpy() =
            [[-1.        ]
     [-0.97979796]
     [-0.959596  ]
     [-0.93939394]
     [-0.9191919 ]
     [-0.8989899 ]
     [-0.8787879 ]
     [-0.85858583]
     [-0.83838385]
     [-0.8181818 ]
     [-0.79797983]
     [-0.7777778 ]
     [-0.75757575]
     [-0.73737377]
     [-0.7171717 ]
     [-0.69696975]
     [-0.6767677 ]
            y.data.numpy() = 
            [[1.021182  ]
     [1.0604763 ]
     [0.94123447]
     [1.0802828 ]
     [0.973209  ]
     [0.9062251 ]
     [0.8753375 ]
     [0.74693096]
     [0.7876559 ]
     [0.74446815]
     [0.7479949 ]
     [0.61663127]
     [0.65863925]
     [0.6573144 ]
     [0.5266536 ]
     [0.63320225]
     [0.4989453 ]
     [0.5051787 ]
     [0.
        '''
    # plt.show()

    print('------ 搭建网络 ------')
    # 使用固定的方式继承并重写 init和forword两个类
    class Net(torch.nn.Module):
        def __init__(self, n_feature, n_hidden, n_output):
            #初始网络的内部结构
            super(Net, self).__init__()
            self.hidden = torch.nn.Linear(n_feature, n_hidden)
            self.predict = torch.nn.Linear(n_hidden, n_output)

        def forward(self, x):
            #一次正向传输过程
            x = F.relu(self.hidden(x))
            x = self.predict(x)
            return x

    net = Net(n_feature=1, n_hidden=1000, n_output=1)
    print('网络结构为：', net)

    print('------ 启动训练 ------')
    loss_func = F.mse_loss
    optimizer = torch.optim.SGD(net.parameters(), lr=0.001)

    #使用数据 进行正向训练，并对Variable变量进行反向梯度传播  启动100次训练
    for t in range(100):
        #使用全量数据 进行正向行走
        prediction = net(x)
        loss = loss_func(prediction, y)
        optimizer.zero_grad() #清除上一梯度
        loss.backward() #反向传播计算梯度
        optimizer.step() #应用梯度

        #间隔一段，对训练过程进行可视化展示
        if t%5 == 0:
            # plt.cla()
            # plt.scatter(x.data.numpy(), y.data.numpy()) #绘制真实曲线
            # plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
            # plt.text(0.5, 0, 'Loss='+ str(loss.item()), fontdict={'size':20, 'color':'red'})
            # plt.pause(0.1)
            print(loss.item())

    # plt.ioff()
    # plt.show()
    print('------ 预测和可视化 ------')

if __name__ == '__main__':
    train()
